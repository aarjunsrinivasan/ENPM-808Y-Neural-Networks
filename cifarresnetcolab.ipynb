{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "cifarresnetcolab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarjunsrinivasan/ENPM-808Y-Neural-Networks/blob/master/cifarresnetcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLn7BkLRdz-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_val=x_train[45000:,:,:,:]\n",
        "y_val=y_train[45000:]\n",
        "x_train=x_train[:45000,:,:,:]\n",
        "y_train=y_train[:45000]\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64).shuffle(10000)\n",
        "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
        "train_dataset = train_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "train_dataset = train_dataset.repeat()\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(5000).shuffle(10000)\n",
        "valid_dataset = valid_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(5000).shuffle(10000)\n",
        "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
        "\n",
        "def res_net_block(input_data, filters, conv_size):\n",
        "      x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
        "      x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
        "      x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "      x = layers.Dropout(0.1)(x)\n",
        "      x = layers.Add()([x, input_data])\n",
        "      x = layers.Activation('relu')(x)\n",
        "      return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1HrTcUniXJK",
        "colab_type": "code",
        "outputId": "04701f64-c485-4e26-9c1a-75ea65a1ad00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAx51Ildz-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "num_res_net_blocks = 20\n",
        "for i in range(num_res_net_blocks):\n",
        "    x = res_net_block(x, 64, 3)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUsx-ViR7BH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "checkpoint_path = \"training_cifar10_2/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX_TP8v8VPjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "      res_net_model = tf.keras.Model(inputs, outputs)\n",
        "      res_net_model.compile(optimizer=keras.optimizers.Adam(),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prltBAORViIU",
        "colab_type": "code",
        "outputId": "69d6f166-2964-4682-bc0a-f46bb0dda839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist=res_net_model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
        "              validation_data=valid_dataset, callbacks=cp_callback)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 1.6931 - acc: 0.3701\n",
            "Epoch 00001: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 8s 40ms/step - loss: 1.6910 - acc: 0.3713 - val_loss: 1.4445 - val_acc: 0.4648\n",
            "Epoch 2/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 1.3079 - acc: 0.5221\n",
            "Epoch 00002: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 1.3074 - acc: 0.5227 - val_loss: 1.5105 - val_acc: 0.4908\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.1347 - acc: 0.5930\n",
            "Epoch 00003: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 1.1347 - acc: 0.5930 - val_loss: 1.0967 - val_acc: 0.6108\n",
            "Epoch 4/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 1.0099 - acc: 0.6437\n",
            "Epoch 00004: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 1.0107 - acc: 0.6434 - val_loss: 0.9566 - val_acc: 0.6650\n",
            "Epoch 5/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.6740\n",
            "Epoch 00005: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.9347 - acc: 0.6740 - val_loss: 1.0210 - val_acc: 0.6480\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.8697 - acc: 0.6945\n",
            "Epoch 00006: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.8697 - acc: 0.6945 - val_loss: 0.9181 - val_acc: 0.6780\n",
            "Epoch 7/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.8304 - acc: 0.7125\n",
            "Epoch 00007: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.8311 - acc: 0.7123 - val_loss: 0.8649 - val_acc: 0.6970\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.7779 - acc: 0.7255\n",
            "Epoch 00008: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.7779 - acc: 0.7255 - val_loss: 0.8498 - val_acc: 0.7092\n",
            "Epoch 9/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.7399 - acc: 0.7430\n",
            "Epoch 00009: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.7400 - acc: 0.7430 - val_loss: 0.7231 - val_acc: 0.7516\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.7521\n",
            "Epoch 00010: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.7100 - acc: 0.7521 - val_loss: 0.7303 - val_acc: 0.7544\n",
            "Epoch 11/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.7717\n",
            "Epoch 00011: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.6706 - acc: 0.7717 - val_loss: 0.7538 - val_acc: 0.7466\n",
            "Epoch 12/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.7721\n",
            "Epoch 00012: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.6601 - acc: 0.7723 - val_loss: 0.6662 - val_acc: 0.7714\n",
            "Epoch 13/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.6439 - acc: 0.7771\n",
            "Epoch 00013: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.6434 - acc: 0.7773 - val_loss: 0.6564 - val_acc: 0.7760\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.6361 - acc: 0.7771\n",
            "Epoch 00014: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.6361 - acc: 0.7771 - val_loss: 0.6866 - val_acc: 0.7712\n",
            "Epoch 15/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.8054\n",
            "Epoch 00015: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.5704 - acc: 0.8052 - val_loss: 0.7166 - val_acc: 0.7650\n",
            "Epoch 16/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.7998\n",
            "Epoch 00016: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.5837 - acc: 0.7998 - val_loss: 0.5810 - val_acc: 0.8012\n",
            "Epoch 17/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.7963\n",
            "Epoch 00017: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.5859 - acc: 0.7961 - val_loss: 0.6496 - val_acc: 0.7818\n",
            "Epoch 18/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8099\n",
            "Epoch 00018: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.5575 - acc: 0.8099 - val_loss: 0.5665 - val_acc: 0.8050\n",
            "Epoch 19/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8138\n",
            "Epoch 00019: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.5366 - acc: 0.8140 - val_loss: 0.5539 - val_acc: 0.8134\n",
            "Epoch 20/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.8189\n",
            "Epoch 00020: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.5295 - acc: 0.8185 - val_loss: 0.5972 - val_acc: 0.7968\n",
            "Epoch 21/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8224\n",
            "Epoch 00021: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.5160 - acc: 0.8223 - val_loss: 0.5601 - val_acc: 0.8074\n",
            "Epoch 22/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8324\n",
            "Epoch 00022: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 8s 38ms/step - loss: 0.4844 - acc: 0.8327 - val_loss: 0.6125 - val_acc: 0.7990\n",
            "Epoch 23/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8266\n",
            "Epoch 00023: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.5036 - acc: 0.8259 - val_loss: 0.5435 - val_acc: 0.8124\n",
            "Epoch 24/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8307\n",
            "Epoch 00024: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.4883 - acc: 0.8308 - val_loss: 0.5011 - val_acc: 0.8308\n",
            "Epoch 25/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.8417\n",
            "Epoch 00025: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.4554 - acc: 0.8420 - val_loss: 0.5500 - val_acc: 0.8162\n",
            "Epoch 26/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8430\n",
            "Epoch 00026: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.4484 - acc: 0.8433 - val_loss: 0.5315 - val_acc: 0.8252\n",
            "Epoch 27/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8375\n",
            "Epoch 00027: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.4746 - acc: 0.8373 - val_loss: 0.5208 - val_acc: 0.8196\n",
            "Epoch 28/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.8519\n",
            "Epoch 00028: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.4312 - acc: 0.8522 - val_loss: 0.5040 - val_acc: 0.8296\n",
            "Epoch 29/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8554\n",
            "Epoch 00029: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.4168 - acc: 0.8554 - val_loss: 0.4761 - val_acc: 0.8374\n",
            "Epoch 30/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8551\n",
            "Epoch 00030: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.4193 - acc: 0.8551 - val_loss: 0.4850 - val_acc: 0.8398\n",
            "Epoch 31/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.8535\n",
            "Epoch 00031: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.4305 - acc: 0.8536 - val_loss: 0.5122 - val_acc: 0.8276\n",
            "Epoch 32/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8573\n",
            "Epoch 00032: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.4104 - acc: 0.8577 - val_loss: 0.4791 - val_acc: 0.8388\n",
            "Epoch 33/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8647\n",
            "Epoch 00033: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3827 - acc: 0.8646 - val_loss: 0.5002 - val_acc: 0.8388\n",
            "Epoch 34/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8619\n",
            "Epoch 00034: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.4004 - acc: 0.8620 - val_loss: 0.5091 - val_acc: 0.8286\n",
            "Epoch 35/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8653\n",
            "Epoch 00035: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3907 - acc: 0.8655 - val_loss: 0.4798 - val_acc: 0.8348\n",
            "Epoch 36/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8746\n",
            "Epoch 00036: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3670 - acc: 0.8744 - val_loss: 0.4694 - val_acc: 0.8496\n",
            "Epoch 37/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8711\n",
            "Epoch 00037: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.3651 - acc: 0.8712 - val_loss: 0.5171 - val_acc: 0.8328\n",
            "Epoch 38/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.3740 - acc: 0.8699\n",
            "Epoch 00038: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3740 - acc: 0.8699 - val_loss: 0.5050 - val_acc: 0.8316\n",
            "Epoch 39/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8787\n",
            "Epoch 00039: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3538 - acc: 0.8788 - val_loss: 0.4576 - val_acc: 0.8494\n",
            "Epoch 40/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8798\n",
            "Epoch 00040: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3439 - acc: 0.8796 - val_loss: 0.4702 - val_acc: 0.8434\n",
            "Epoch 41/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8822\n",
            "Epoch 00041: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3341 - acc: 0.8822 - val_loss: 0.4770 - val_acc: 0.8452\n",
            "Epoch 42/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8805\n",
            "Epoch 00042: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.3324 - acc: 0.8805 - val_loss: 0.4935 - val_acc: 0.8418\n",
            "Epoch 43/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8879\n",
            "Epoch 00043: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3142 - acc: 0.8876 - val_loss: 0.4893 - val_acc: 0.8470\n",
            "Epoch 44/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8876\n",
            "Epoch 00044: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.3233 - acc: 0.8879 - val_loss: 0.4827 - val_acc: 0.8426\n",
            "Epoch 45/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8892\n",
            "Epoch 00045: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.3381 - acc: 0.8889 - val_loss: 0.5161 - val_acc: 0.8378\n",
            "Epoch 46/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8872\n",
            "Epoch 00046: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.3249 - acc: 0.8870 - val_loss: 0.4513 - val_acc: 0.8524\n",
            "Epoch 47/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.8995\n",
            "Epoch 00047: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2878 - acc: 0.8997 - val_loss: 0.5672 - val_acc: 0.8260\n",
            "Epoch 48/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.8973\n",
            "Epoch 00048: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2949 - acc: 0.8971 - val_loss: 0.4781 - val_acc: 0.8474\n",
            "Epoch 49/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8874\n",
            "Epoch 00049: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.3202 - acc: 0.8874 - val_loss: 0.4397 - val_acc: 0.8520\n",
            "Epoch 50/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9026\n",
            "Epoch 00050: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2784 - acc: 0.9027 - val_loss: 0.4671 - val_acc: 0.8522\n",
            "Epoch 51/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9030\n",
            "Epoch 00051: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2753 - acc: 0.9030 - val_loss: 0.4903 - val_acc: 0.8472\n",
            "Epoch 52/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8971\n",
            "Epoch 00052: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.3042 - acc: 0.8968 - val_loss: 0.4654 - val_acc: 0.8524\n",
            "Epoch 53/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.8990\n",
            "Epoch 00053: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2915 - acc: 0.8991 - val_loss: 0.4578 - val_acc: 0.8532\n",
            "Epoch 54/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9110\n",
            "Epoch 00054: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2572 - acc: 0.9108 - val_loss: 0.4737 - val_acc: 0.8532\n",
            "Epoch 55/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9034\n",
            "Epoch 00055: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2690 - acc: 0.9036 - val_loss: 0.4531 - val_acc: 0.8600\n",
            "Epoch 56/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.8990\n",
            "Epoch 00056: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2829 - acc: 0.8993 - val_loss: 0.4272 - val_acc: 0.8638\n",
            "Epoch 57/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9140\n",
            "Epoch 00057: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2434 - acc: 0.9139 - val_loss: 0.4313 - val_acc: 0.8644\n",
            "Epoch 58/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9122\n",
            "Epoch 00058: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 37ms/step - loss: 0.2497 - acc: 0.9120 - val_loss: 0.4404 - val_acc: 0.8554\n",
            "Epoch 59/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9110\n",
            "Epoch 00059: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2544 - acc: 0.9111 - val_loss: 0.4400 - val_acc: 0.8652\n",
            "Epoch 60/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9096\n",
            "Epoch 00060: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2507 - acc: 0.9097 - val_loss: 0.4247 - val_acc: 0.8660\n",
            "Epoch 61/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9276\n",
            "Epoch 00061: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2070 - acc: 0.9280 - val_loss: 0.4463 - val_acc: 0.8676\n",
            "Epoch 62/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9161\n",
            "Epoch 00062: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2390 - acc: 0.9161 - val_loss: 0.5009 - val_acc: 0.8514\n",
            "Epoch 63/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.2433 - acc: 0.9122\n",
            "Epoch 00063: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2433 - acc: 0.9122 - val_loss: 0.4546 - val_acc: 0.8654\n",
            "Epoch 64/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9190\n",
            "Epoch 00064: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2296 - acc: 0.9192 - val_loss: 0.4854 - val_acc: 0.8580\n",
            "Epoch 65/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9271\n",
            "Epoch 00065: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2079 - acc: 0.9273 - val_loss: 0.4457 - val_acc: 0.8668\n",
            "Epoch 66/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9181\n",
            "Epoch 00066: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2334 - acc: 0.9184 - val_loss: 0.4515 - val_acc: 0.8600\n",
            "Epoch 67/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9189\n",
            "Epoch 00067: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2302 - acc: 0.9191 - val_loss: 0.4824 - val_acc: 0.8518\n",
            "Epoch 68/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9322\n",
            "Epoch 00068: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1963 - acc: 0.9323 - val_loss: 0.4794 - val_acc: 0.8560\n",
            "Epoch 69/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9200\n",
            "Epoch 00069: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.2177 - acc: 0.9199 - val_loss: 0.4826 - val_acc: 0.8612\n",
            "Epoch 70/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9240\n",
            "Epoch 00070: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2121 - acc: 0.9239 - val_loss: 0.4990 - val_acc: 0.8562\n",
            "Epoch 71/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9326\n",
            "Epoch 00071: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1971 - acc: 0.9325 - val_loss: 0.4948 - val_acc: 0.8588\n",
            "Epoch 72/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9305\n",
            "Epoch 00072: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1997 - acc: 0.9304 - val_loss: 0.5475 - val_acc: 0.8456\n",
            "Epoch 73/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9273\n",
            "Epoch 00073: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2068 - acc: 0.9274 - val_loss: 0.4660 - val_acc: 0.8692\n",
            "Epoch 74/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9192\n",
            "Epoch 00074: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2273 - acc: 0.9194 - val_loss: 0.4607 - val_acc: 0.8598\n",
            "Epoch 75/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9392\n",
            "Epoch 00075: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1677 - acc: 0.9393 - val_loss: 0.4803 - val_acc: 0.8588\n",
            "Epoch 76/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9323\n",
            "Epoch 00076: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1880 - acc: 0.9324 - val_loss: 0.4966 - val_acc: 0.8616\n",
            "Epoch 77/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9295\n",
            "Epoch 00077: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.2074 - acc: 0.9294 - val_loss: 0.4666 - val_acc: 0.8668\n",
            "Epoch 78/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9376\n",
            "Epoch 00078: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1767 - acc: 0.9375 - val_loss: 0.4531 - val_acc: 0.8706\n",
            "Epoch 79/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9415\n",
            "Epoch 00079: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1676 - acc: 0.9416 - val_loss: 0.4839 - val_acc: 0.8672\n",
            "Epoch 80/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9349\n",
            "Epoch 00080: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1835 - acc: 0.9348 - val_loss: 0.5390 - val_acc: 0.8516\n",
            "Epoch 81/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9283\n",
            "Epoch 00081: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.2035 - acc: 0.9286 - val_loss: 0.4722 - val_acc: 0.8628\n",
            "Epoch 82/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9430\n",
            "Epoch 00082: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1577 - acc: 0.9430 - val_loss: 0.4835 - val_acc: 0.8644\n",
            "Epoch 83/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9383\n",
            "Epoch 00083: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1815 - acc: 0.9382 - val_loss: 0.4640 - val_acc: 0.8680\n",
            "Epoch 84/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9412\n",
            "Epoch 00084: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1702 - acc: 0.9412 - val_loss: 0.4683 - val_acc: 0.8676\n",
            "Epoch 85/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9409\n",
            "Epoch 00085: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1635 - acc: 0.9412 - val_loss: 0.4858 - val_acc: 0.8656\n",
            "Epoch 86/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9424\n",
            "Epoch 00086: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1593 - acc: 0.9420 - val_loss: 0.4703 - val_acc: 0.8738\n",
            "Epoch 87/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9405\n",
            "Epoch 00087: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1637 - acc: 0.9404 - val_loss: 0.4860 - val_acc: 0.8630\n",
            "Epoch 88/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9449\n",
            "Epoch 00088: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1578 - acc: 0.9448 - val_loss: 0.4634 - val_acc: 0.8702\n",
            "Epoch 89/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9484\n",
            "Epoch 00089: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1415 - acc: 0.9484 - val_loss: 0.5119 - val_acc: 0.8648\n",
            "Epoch 90/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9452\n",
            "Epoch 00090: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1515 - acc: 0.9451 - val_loss: 0.5038 - val_acc: 0.8628\n",
            "Epoch 91/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9469\n",
            "Epoch 00091: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1499 - acc: 0.9470 - val_loss: 0.4814 - val_acc: 0.8668\n",
            "Epoch 92/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9473\n",
            "Epoch 00092: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1470 - acc: 0.9476 - val_loss: 0.5021 - val_acc: 0.8662\n",
            "Epoch 93/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9528\n",
            "Epoch 00093: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1326 - acc: 0.9527 - val_loss: 0.5125 - val_acc: 0.8638\n",
            "Epoch 94/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9467\n",
            "Epoch 00094: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1496 - acc: 0.9467 - val_loss: 0.5010 - val_acc: 0.8724\n",
            "Epoch 95/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.1545 - acc: 0.9439\n",
            "Epoch 00095: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1545 - acc: 0.9439 - val_loss: 0.4812 - val_acc: 0.8726\n",
            "Epoch 96/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9558\n",
            "Epoch 00096: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1240 - acc: 0.9556 - val_loss: 0.4708 - val_acc: 0.8776\n",
            "Epoch 97/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.1285 - acc: 0.9534\n",
            "Epoch 00097: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1285 - acc: 0.9534 - val_loss: 0.5369 - val_acc: 0.8626\n",
            "Epoch 98/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9477\n",
            "Epoch 00098: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1465 - acc: 0.9475 - val_loss: 0.4934 - val_acc: 0.8716\n",
            "Epoch 99/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9478\n",
            "Epoch 00099: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 35ms/step - loss: 0.1428 - acc: 0.9478 - val_loss: 0.5134 - val_acc: 0.8702\n",
            "Epoch 100/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9560\n",
            "Epoch 00100: saving model to training_cifar10_2/cp.ckpt\n",
            "200/200 [==============================] - 7s 36ms/step - loss: 0.1197 - acc: 0.9559 - val_loss: 0.5100 - val_acc: 0.8714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzhOJ-9S45BG",
        "colab_type": "code",
        "outputId": "fdf8c62d-2009-47d2-f021-241da4c9a905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGXZ0JqB5qRR",
        "colab_type": "code",
        "outputId": "7589e80a-0e1f-4172-a2b3-662cac32f93a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "f, ax = plt.subplots()\n",
        "ax.plot([None] + hist.history['loss'], 'o-')\n",
        "ax.plot([None] + hist.history['val_loss'], 'x-')\n",
        "# Plot legend and use the best location automatically: loc = 0.\n",
        "ax.legend(['Train Loss', 'Test Loss'], loc = 0)\n",
        "ax.set_title('Training/Test Loss per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss') \n",
        "# plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9w7xLDS5G5r+IAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo20Of64gH2D",
        "colab_type": "code",
        "outputId": "7b2af51a-695a-447c-c233-8e4f045ae4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "res_net_model.evaluate(test_dataset)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 144ms/step - loss: 0.5332 - acc: 0.8628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.533187747001648, 0.8628000020980835]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3PxRRLDK30D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}